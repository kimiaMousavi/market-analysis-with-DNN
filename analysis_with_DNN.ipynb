{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q7FfDX3cJwU3"
   },
   "source": [
    "# precoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6FHlGDsI-8HR"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sSOPUjo9-JVa"
   },
   "outputs": [],
   "source": [
    "# Code to read csv file into Colaboratory:\n",
    "\n",
    "!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "# Authenticate and create the PyDrive client.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uwl-rabkKaqL"
   },
   "source": [
    "importing data from test.csv and train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "srbqApVi-MxE"
   },
   "outputs": [],
   "source": [
    "train_link = 'https://drive.google.com/open?id=1-1u1z1Jwh-NHGwHLcvqDe99qLRNtU1ty' # The shareable link\n",
    "test_link='https://drive.google.com/open?id=1HRUrR1J5sNMkPKga4GcHR1Lkmon6LdBn'\n",
    "\n",
    "train_fluff, train_id = train_link.split('=')\n",
    "test_fluff, test_id = test_link.split('=')\n",
    "\n",
    "\n",
    "train_downloaded = drive.CreateFile({'id':train_id}) \n",
    "train_downloaded.GetContentFile('train.csv')  \n",
    "train_df = pd.read_csv('train.csv')\n",
    "\n",
    "test_downloaded = drive.CreateFile({'id':test_id}) \n",
    "test_downloaded.GetContentFile('test.csv')  \n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Dataset is now stored in a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BBoTK3MSKjWk"
   },
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WzJ6tVxe48L5"
   },
   "source": [
    "we need some preprocesing on data to make it suitable for neural network so first we drop columns that we dont need(cause they are not in test data) and rename the others to similar names in both train and test.\n",
    "\n",
    "another thing that is necessary is to omit year prefix from Log_Dates, that is \"1395/\" and \"1396/\" ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4eXKfvIjKldv"
   },
   "outputs": [],
   "source": [
    "result_df=test_df\n",
    "train_df.drop([\"Log_Time\"], axis=1, inplace=True)\n",
    "train_df.drop([\"AL\"], axis=1, inplace=True)\n",
    "train_df.drop([\"Departure_Time\"], axis=1, inplace=True)\n",
    "train_df.drop([\"Departure_Date\"], axis=1, inplace=True)\n",
    "train_df.drop([\"Price\"], axis=1, inplace=True)\n",
    "train_df.rename(columns={'FROM': 'From'}, inplace=True)  # renaming\n",
    "train_df.rename(columns={'TO': 'To'}, inplace=True)  # renaming\n",
    "train_df = train_df.replace(to_replace=\"1395/\", value='', regex=True)\n",
    "train_df = train_df.replace(to_replace=\"1396/\", value='', regex=True)\n",
    "test_df = test_df.replace(to_replace=\"1396/\", value='', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ELdGrzrp6dE3"
   },
   "source": [
    "we should handle missing values before passing parameters to model, so our solution is to drop missing values in 3 columns below from test and train data frames.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8hBKiWEVKykR"
   },
   "outputs": [],
   "source": [
    "\n",
    "train_df = train_df.dropna(subset=['Log_Date', 'From', 'To'])\n",
    "test_df = test_df.dropna(subset=['Log_Date', 'From', 'To'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fJDqp2Sl-HLs"
   },
   "source": [
    "we have to add a new column that is count of records with same (data, from, to) features .\n",
    "\n",
    "to perform this action we use groupby method and put the sum result in sales column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ijZl2FziK2tw"
   },
   "outputs": [],
   "source": [
    "\n",
    "train_df = train_df.groupby(['Log_Date', 'From', 'To']).size().reset_index()  # counting same (date,from,to) records\n",
    "train_df.rename(columns={0: 'Sales'}, inplace=True)  # renaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wy21QkjA_nB1"
   },
   "source": [
    "if we omit year prefix from Log_Date column and then groupby the result, the Sales result would be wrong in first 6 months of year,\n",
    "because our train data set contains records of one and half of a year so the sales feature would be calculated twice as real amount.\n",
    "so our solution is to divide the Sales feature of first 6 month by 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-ycHUng2K7Tq"
   },
   "outputs": [],
   "source": [
    "\n",
    "train_df.loc[train_df[\"Log_Date\"].str.startswith('01/'), 'Sales'] /= 2\n",
    "train_df.loc[train_df[\"Log_Date\"].str.startswith('02/'), 'Sales'] /= 2\n",
    "train_df.loc[train_df[\"Log_Date\"].str.startswith('03/'), 'Sales'] /= 2\n",
    "train_df.loc[train_df[\"Log_Date\"].str.startswith('04/'), 'Sales'] /= 2\n",
    "train_df.loc[train_df[\"Log_Date\"].str.startswith('05/'), 'Sales'] /= 2\n",
    "train_df.loc[train_df[\"Log_Date\"].str.startswith('06/'), 'Sales'] /= 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k7kRErwk75rG"
   },
   "source": [
    "there are several encoding methods for every feature type.\n",
    "one of the most optimal and famous encoding methods for time is sin/cos encoding.\n",
    "this encoding method extracs periodic features from time.\n",
    "the code bellow shows how we emplemented this kind of encoding in our code but we decided to use one-hot encoding because of performance issues .\n",
    "the nature of this problem caused this exception.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lYbmV82_N9GP"
   },
   "outputs": [],
   "source": [
    "\n",
    "# my_df.Log_Date = my_df.Log_Date.astype(int)\n",
    "# my_df['sin_time'] = np.sin(2 * np.pi * my_df.Log_Date / 31)  # periodic nature of month\n",
    "# my_df['cos_time'] = np.cos(2 * np.pi * my_df.Log_Date / 31)\n",
    "# plt.scatter(my_df.sin_time,my_df.cos_time)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bwcL-UlMCkVs"
   },
   "source": [
    "one of important part of preprocessing is encoding datas, as described before we decided to use one-hot encoding for date,from and to features.\n",
    "if we encode test and train separatly, the resulting columns would be different because of different values in them.\n",
    "to solve this issue we combined test and train data  and then encoded the combined data frame. we can separate test and train data after encoding with the predefined boolean feature called \"train\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "YVqyq9kYLIpq",
    "outputId": "84999862-2248-4934-ad97-04325dee80cd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:3940: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_df['train'] = 1  \n",
    "test_df['train'] = 0\n",
    "\n",
    "combined = pd.concat([train_df, test_df])\n",
    "\n",
    "combined = pd.get_dummies(combined, columns=['Log_Date', 'From', 'To'])   # one_hot encoding of given columns\n",
    "\n",
    "train_df = combined[combined[\"train\"] == 1]  \n",
    "\n",
    "test_df = combined[combined[\"train\"] == 0]\n",
    "\n",
    "train_df.drop([\"train\"], axis=1, inplace=True)\n",
    "test_df.drop([\"train\"], axis=1, inplace=True)\n",
    "\n",
    "test_df.drop([\"Sales\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JP17n6-LEpBQ"
   },
   "source": [
    "we can increase performance by shuffling data frame order before passing it to model.\n",
    "this performance increment happens because when we used group by method, the result was ordered by Log_Date  and this has bad effect on our learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WrVEYsf6LPCT"
   },
   "outputs": [],
   "source": [
    "train_df = train_df.reindex(np.random.permutation(train_df.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2_PEtfJoGEA3"
   },
   "source": [
    "Next, we need to split up our dataset into inputs (train_X) and our target (train_y).\n",
    "\n",
    "Our input will be every column except ‘sales’ because ‘sales’ is what we will be attempting to predict so ‘sales’ is our target (train_y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yiPzWW_yLVob"
   },
   "outputs": [],
   "source": [
    "\n",
    "train_x = train_df.drop(columns=['Sales'])  # splitting data\n",
    "train_y = train_df[['Sales']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5ev-W-InGS05"
   },
   "source": [
    "# building and compiling the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NW8CgtNoGaTM"
   },
   "source": [
    "The model type that we use is Sequential with ‘add()’ function we add 4 layer to our model (‘Dense’ is the layer type in a dense layer, all nodes in the previous layer connect to the nodes in the current layer).  we find model's optimal configurations experimentaly.\n",
    "The first layer needs an input shape and the last layer is the output layer .\n",
    "\n",
    "we used linear activation function for last layer because there shouldnt be upper and higher boundries for last layer.\n",
    "\n",
    " Compiling the model takes two parameters: optimizer and loss.The optimizer controls the learning rate. We use ‘adam’.For  loss function, we use ‘mean_absolute_percentage_error’ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gdXcGCbBLiSh"
   },
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential()\n",
    "n_cols = train_x.shape[1]\n",
    "model.add(Dense(100, activation='softmax', input_shape=(n_cols,)))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_absolute_percentage_error')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tJ314GAQH-6_"
   },
   "source": [
    "# training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-96n3gABIEZW"
   },
   "source": [
    "Now we  train our model. \n",
    "\n",
    "To train, we  use the ‘fit()’ function on our training data frame.\n",
    "\n",
    "We set the validation split to 0.2, which means that 20% of the training data we provide in the model will be set aside for testing model performance.\n",
    "\n",
    "The number of epochs is the number of times the model will cycle through the data.\n",
    "the default batch_size amount is 1, but it is very fine-grained for our problem, so we set it to 128, this will imoprove performance and learning time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2170
    },
    "colab_type": "code",
    "id": "lALp4j-QLlKQ",
    "outputId": "88c9f29e-9ed3-4cce-a22e-e1249834933a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48662 samples, validate on 12166 samples\n",
      "Epoch 1/200\n",
      "48662/48662 [==============================] - 3s 53us/step - loss: 71.6437 - val_loss: 63.8833\n",
      "Epoch 2/200\n",
      "48662/48662 [==============================] - 2s 39us/step - loss: 64.0754 - val_loss: 63.6071\n",
      "Epoch 3/200\n",
      "48662/48662 [==============================] - 2s 40us/step - loss: 63.6119 - val_loss: 62.9712\n",
      "Epoch 4/200\n",
      "48662/48662 [==============================] - 2s 40us/step - loss: 62.4807 - val_loss: 61.7165\n",
      "Epoch 5/200\n",
      "48662/48662 [==============================] - 2s 41us/step - loss: 61.2771 - val_loss: 60.9234\n",
      "Epoch 6/200\n",
      "48662/48662 [==============================] - 2s 42us/step - loss: 60.6474 - val_loss: 60.6811\n",
      "Epoch 7/200\n",
      "48662/48662 [==============================] - 2s 40us/step - loss: 60.3582 - val_loss: 60.4098\n",
      "Epoch 8/200\n",
      "48662/48662 [==============================] - 2s 40us/step - loss: 60.0913 - val_loss: 60.2495\n",
      "Epoch 9/200\n",
      "48662/48662 [==============================] - 2s 40us/step - loss: 59.8084 - val_loss: 60.1570\n",
      "Epoch 10/200\n",
      "48662/48662 [==============================] - 2s 40us/step - loss: 59.4296 - val_loss: 59.7081\n",
      "Epoch 11/200\n",
      "48662/48662 [==============================] - 2s 40us/step - loss: 58.9381 - val_loss: 59.4633\n",
      "Epoch 12/200\n",
      "48662/48662 [==============================] - 2s 40us/step - loss: 58.2615 - val_loss: 58.6812\n",
      "Epoch 13/200\n",
      "48662/48662 [==============================] - 2s 39us/step - loss: 57.3710 - val_loss: 58.0337\n",
      "Epoch 14/200\n",
      "48662/48662 [==============================] - 2s 39us/step - loss: 56.2044 - val_loss: 57.1811\n",
      "Epoch 15/200\n",
      "48662/48662 [==============================] - 2s 39us/step - loss: 54.9130 - val_loss: 56.3044\n",
      "Epoch 16/200\n",
      "48662/48662 [==============================] - 2s 39us/step - loss: 53.4870 - val_loss: 55.0688\n",
      "Epoch 17/200\n",
      "48662/48662 [==============================] - 2s 40us/step - loss: 52.0000 - val_loss: 53.9947\n",
      "Epoch 18/200\n",
      "48662/48662 [==============================] - 2s 40us/step - loss: 50.6641 - val_loss: 53.0131\n",
      "Epoch 19/200\n",
      "48662/48662 [==============================] - 2s 40us/step - loss: 49.5238 - val_loss: 52.0719\n",
      "Epoch 20/200\n",
      "48662/48662 [==============================] - 2s 40us/step - loss: 48.5232 - val_loss: 51.2652\n",
      "Epoch 21/200\n",
      "48662/48662 [==============================] - 2s 40us/step - loss: 47.5557 - val_loss: 50.8837\n",
      "Epoch 22/200\n",
      "48662/48662 [==============================] - 2s 39us/step - loss: 46.5654 - val_loss: 49.7534\n",
      "Epoch 23/200\n",
      "48662/48662 [==============================] - 2s 40us/step - loss: 45.3978 - val_loss: 48.8060\n",
      "Epoch 24/200\n",
      "48662/48662 [==============================] - 2s 40us/step - loss: 44.1379 - val_loss: 47.6334\n",
      "Epoch 25/200\n",
      "48662/48662 [==============================] - 2s 40us/step - loss: 42.9814 - val_loss: 46.7901\n",
      "Epoch 26/200\n",
      "48662/48662 [==============================] - 2s 40us/step - loss: 41.8994 - val_loss: 45.9805\n",
      "Epoch 27/200\n",
      "48662/48662 [==============================] - 2s 41us/step - loss: 40.9042 - val_loss: 45.1979\n",
      "Epoch 28/200\n",
      "48662/48662 [==============================] - 2s 42us/step - loss: 40.1581 - val_loss: 44.8198\n",
      "Epoch 29/200\n",
      "48662/48662 [==============================] - 2s 40us/step - loss: 39.4746 - val_loss: 44.7148\n",
      "Epoch 30/200\n",
      "48662/48662 [==============================] - 2s 40us/step - loss: 38.9083 - val_loss: 44.9229\n",
      "Epoch 31/200\n",
      "48662/48662 [==============================] - 2s 41us/step - loss: 38.4301 - val_loss: 44.2325\n",
      "Epoch 32/200\n",
      "48662/48662 [==============================] - 2s 40us/step - loss: 37.9320 - val_loss: 43.8565\n",
      "Epoch 33/200\n",
      "48662/48662 [==============================] - 2s 39us/step - loss: 37.4645 - val_loss: 43.7412\n",
      "Epoch 34/200\n",
      "48662/48662 [==============================] - 2s 40us/step - loss: 37.0490 - val_loss: 43.3462\n",
      "Epoch 35/200\n",
      "48662/48662 [==============================] - 2s 41us/step - loss: 36.6753 - val_loss: 42.9411\n",
      "Epoch 36/200\n",
      "48662/48662 [==============================] - 2s 40us/step - loss: 36.3031 - val_loss: 43.0961\n",
      "Epoch 37/200\n",
      "48662/48662 [==============================] - 2s 40us/step - loss: 35.9494 - val_loss: 43.1305\n",
      "Epoch 38/200\n",
      "48662/48662 [==============================] - 2s 40us/step - loss: 35.6217 - val_loss: 42.6199\n",
      "Epoch 39/200\n",
      "48662/48662 [==============================] - 2s 40us/step - loss: 35.3423 - val_loss: 42.7327\n",
      "Epoch 40/200\n",
      "48662/48662 [==============================] - 2s 40us/step - loss: 35.0721 - val_loss: 42.6483\n",
      "Epoch 41/200\n",
      "48662/48662 [==============================] - 2s 39us/step - loss: 34.7931 - val_loss: 42.3549\n",
      "Epoch 42/200\n",
      "48662/48662 [==============================] - 2s 40us/step - loss: 34.5122 - val_loss: 42.9611\n",
      "Epoch 43/200\n",
      "48662/48662 [==============================] - 2s 39us/step - loss: 34.2515 - val_loss: 43.4338\n",
      "Epoch 44/200\n",
      "48662/48662 [==============================] - 2s 41us/step - loss: 33.9475 - val_loss: 42.4974\n",
      "Epoch 45/200\n",
      "48662/48662 [==============================] - 2s 40us/step - loss: 33.6959 - val_loss: 43.3320\n",
      "Epoch 46/200\n",
      "48662/48662 [==============================] - 2s 41us/step - loss: 33.4255 - val_loss: 42.6562\n",
      "Epoch 47/200\n",
      "48662/48662 [==============================] - 2s 41us/step - loss: 33.2168 - val_loss: 42.5209\n",
      "Epoch 48/200\n",
      "48662/48662 [==============================] - 2s 40us/step - loss: 32.9299 - val_loss: 43.0844\n",
      "Epoch 49/200\n",
      "48662/48662 [==============================] - 2s 40us/step - loss: 32.7212 - val_loss: 42.6972\n",
      "Epoch 50/200\n",
      "48662/48662 [==============================] - 2s 40us/step - loss: 32.4392 - val_loss: 42.4774\n",
      "Epoch 51/200\n",
      "48662/48662 [==============================] - 2s 40us/step - loss: 32.2037 - val_loss: 41.9013\n",
      "Epoch 52/200\n",
      "48662/48662 [==============================] - 2s 40us/step - loss: 32.0161 - val_loss: 42.5936\n",
      "Epoch 53/200\n",
      "48662/48662 [==============================] - 2s 43us/step - loss: 31.7767 - val_loss: 42.5463\n",
      "Epoch 54/200\n",
      "48662/48662 [==============================] - 2s 43us/step - loss: 31.5479 - val_loss: 42.9932\n",
      "Epoch 55/200\n",
      "48662/48662 [==============================] - 2s 43us/step - loss: 31.3175 - val_loss: 42.1754\n",
      "Epoch 56/200\n",
      "48662/48662 [==============================] - 2s 43us/step - loss: 31.1317 - val_loss: 43.3158\n",
      "Epoch 57/200\n",
      "48662/48662 [==============================] - 2s 43us/step - loss: 30.8873 - val_loss: 43.3827\n",
      "Epoch 58/200\n",
      "48662/48662 [==============================] - 2s 43us/step - loss: 30.6750 - val_loss: 42.3618\n",
      "Epoch 59/200\n",
      "48662/48662 [==============================] - 2s 39us/step - loss: 30.4944 - val_loss: 43.1140\n",
      "Epoch 60/200\n",
      "48662/48662 [==============================] - 2s 40us/step - loss: 30.2414 - val_loss: 42.5119\n",
      "Epoch 61/200\n",
      "48662/48662 [==============================] - 2s 39us/step - loss: 30.0765 - val_loss: 42.3076\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9e5717ec18>"
      ]
     },
     "execution_count": 55,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "early_stopping_monitor = EarlyStopping(patience=10)\n",
    "\n",
    "model.fit(train_x, train_y, validation_split=0.2, batch_size=128, epochs=200, callbacks=[early_stopping_monitor])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nWlQmWflI4v0"
   },
   "source": [
    "# predicting test values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5vsJjvJqI-Wz"
   },
   "source": [
    "now our model is built and trained so it is ready to get output from it.\n",
    "\n",
    "we can use predict method and pass test data frame to model and model will return result.\n",
    "the results are in float format so we round them to nearest integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ziddI__Y-NUf"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "test_predictions = model.predict(test_df).round()\n",
    "# print(test_predictions[0:10])\n",
    "result_df['Sales']=test_predictions\n",
    "result_df.to_csv(r'result.csv')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [
    "Q7FfDX3cJwU3",
    "BBoTK3MSKjWk",
    "5ev-W-InGS05",
    "tJ314GAQH-6_",
    "nWlQmWflI4v0"
   ],
   "name": "ZNUAI_PH_4_DNN_natural-Intelligence.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
